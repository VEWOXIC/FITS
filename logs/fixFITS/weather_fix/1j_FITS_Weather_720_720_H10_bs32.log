Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=70, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=70, out_features=140, bias=True)
    (1): Linear(in_features=70, out_features=140, bias=True)
    (2): Linear(in_features=70, out_features=140, bias=True)
    (3): Linear(in_features=70, out_features=140, bias=True)
    (4): Linear(in_features=70, out_features=140, bias=True)
    (5): Linear(in_features=70, out_features=140, bias=True)
    (6): Linear(in_features=70, out_features=140, bias=True)
    (7): Linear(in_features=70, out_features=140, bias=True)
    (8): Linear(in_features=70, out_features=140, bias=True)
    (9): Linear(in_features=70, out_features=140, bias=True)
    (10): Linear(in_features=70, out_features=140, bias=True)
    (11): Linear(in_features=70, out_features=140, bias=True)
    (12): Linear(in_features=70, out_features=140, bias=True)
    (13): Linear(in_features=70, out_features=140, bias=True)
    (14): Linear(in_features=70, out_features=140, bias=True)
    (15): Linear(in_features=70, out_features=140, bias=True)
    (16): Linear(in_features=70, out_features=140, bias=True)
    (17): Linear(in_features=70, out_features=140, bias=True)
    (18): Linear(in_features=70, out_features=140, bias=True)
    (19): Linear(in_features=70, out_features=140, bias=True)
    (20): Linear(in_features=70, out_features=140, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  13171200.0
params:  208740.0
Trainable parameters:  208740
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8044301
	speed: 0.2067s/iter; left time: 5695.2444s
	iters: 200, epoch: 1 | loss: 0.7097409
	speed: 0.2106s/iter; left time: 5780.7586s
	iters: 300, epoch: 1 | loss: 0.6344911
	speed: 0.2325s/iter; left time: 6358.4862s
	iters: 400, epoch: 1 | loss: 0.5885920
	speed: 0.2302s/iter; left time: 6272.1465s
	iters: 500, epoch: 1 | loss: 0.5474780
	speed: 0.2056s/iter; left time: 5582.7579s
Epoch: 1 cost time: 119.08701157569885
Epoch: 1, Steps: 553 | Train Loss: 0.6879347 Vali Loss: 0.6174577 Test Loss: 0.3217325
Validation loss decreased (inf --> 0.617458).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.7144024
	speed: 0.9927s/iter; left time: 26799.8754s
	iters: 200, epoch: 2 | loss: 0.4825799
	speed: 0.2438s/iter; left time: 6557.8746s
	iters: 300, epoch: 2 | loss: 0.6907299
	speed: 0.2398s/iter; left time: 6425.7696s
	iters: 400, epoch: 2 | loss: 0.5705237
	speed: 0.2386s/iter; left time: 6371.2538s
	iters: 500, epoch: 2 | loss: 0.6459236
	speed: 0.2326s/iter; left time: 6186.9408s
Epoch: 2 cost time: 132.93079137802124
Epoch: 2, Steps: 553 | Train Loss: 0.5758130 Vali Loss: 0.6045932 Test Loss: 0.3155768
Validation loss decreased (0.617458 --> 0.604593).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5830278
	speed: 1.2512s/iter; left time: 33088.8523s
	iters: 200, epoch: 3 | loss: 0.5900827
	speed: 0.2655s/iter; left time: 6994.9513s
	iters: 300, epoch: 3 | loss: 0.5336768
	speed: 0.2630s/iter; left time: 6901.2188s
	iters: 400, epoch: 3 | loss: 0.6252362
	speed: 0.2360s/iter; left time: 6169.7313s
	iters: 500, epoch: 3 | loss: 0.6322173
	speed: 0.2394s/iter; left time: 6233.8845s
Epoch: 3 cost time: 140.16633677482605
Epoch: 3, Steps: 553 | Train Loss: 0.5668810 Vali Loss: 0.6024516 Test Loss: 0.3135782
Validation loss decreased (0.604593 --> 0.602452).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4327554
	speed: 1.1148s/iter; left time: 28864.7959s
	iters: 200, epoch: 4 | loss: 0.6177879
	speed: 0.1436s/iter; left time: 3703.0039s
	iters: 300, epoch: 4 | loss: 0.5313635
	speed: 0.1195s/iter; left time: 3071.4303s
	iters: 400, epoch: 4 | loss: 0.5840181
	speed: 0.1222s/iter; left time: 3126.2316s
	iters: 500, epoch: 4 | loss: 0.4279472
	speed: 0.1325s/iter; left time: 3377.5816s
Epoch: 4 cost time: 81.44778251647949
Epoch: 4, Steps: 553 | Train Loss: 0.5650051 Vali Loss: 0.6000813 Test Loss: 0.3122967
Validation loss decreased (0.602452 --> 0.600081).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4602176
	speed: 0.8554s/iter; left time: 21673.9026s
	iters: 200, epoch: 5 | loss: 0.6833277
	speed: 0.1902s/iter; left time: 4799.7908s
	iters: 300, epoch: 5 | loss: 0.5580639
	speed: 0.1874s/iter; left time: 4711.0316s
	iters: 400, epoch: 5 | loss: 0.4908301
	speed: 0.1899s/iter; left time: 4753.7420s
	iters: 500, epoch: 5 | loss: 0.5710062
	speed: 0.1790s/iter; left time: 4464.2313s
Epoch: 5 cost time: 104.76799511909485
Epoch: 5, Steps: 553 | Train Loss: 0.5638004 Vali Loss: 0.5982038 Test Loss: 0.3111158
Validation loss decreased (0.600081 --> 0.598204).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5599698
	speed: 0.8393s/iter; left time: 20802.2392s
	iters: 200, epoch: 6 | loss: 0.4992825
	speed: 0.1559s/iter; left time: 3848.4335s
	iters: 300, epoch: 6 | loss: 0.5027092
	speed: 0.0973s/iter; left time: 2393.1651s
	iters: 400, epoch: 6 | loss: 0.5714244
	speed: 0.0891s/iter; left time: 2180.8283s
	iters: 500, epoch: 6 | loss: 0.5680634
	speed: 0.0894s/iter; left time: 2180.3115s
Epoch: 6 cost time: 66.60122179985046
Epoch: 6, Steps: 553 | Train Loss: 0.5628517 Vali Loss: 0.5963978 Test Loss: 0.3103568
Validation loss decreased (0.598204 --> 0.596398).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5104360
	speed: 0.8505s/iter; left time: 20610.2882s
	iters: 200, epoch: 7 | loss: 0.5852817
	speed: 0.2041s/iter; left time: 4925.3807s
	iters: 300, epoch: 7 | loss: 0.4535050
	speed: 0.2047s/iter; left time: 4919.0336s
	iters: 400, epoch: 7 | loss: 0.5617047
	speed: 0.2297s/iter; left time: 5497.6542s
	iters: 500, epoch: 7 | loss: 0.4837149
	speed: 0.2354s/iter; left time: 5610.6893s
Epoch: 7 cost time: 121.64611792564392
Epoch: 7, Steps: 553 | Train Loss: 0.5621622 Vali Loss: 0.5965281 Test Loss: 0.3101959
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5606779
	speed: 0.9923s/iter; left time: 23496.5122s
	iters: 200, epoch: 8 | loss: 0.6840612
	speed: 0.1673s/iter; left time: 3943.9679s
	iters: 300, epoch: 8 | loss: 0.4844505
	speed: 0.2007s/iter; left time: 4712.7762s
	iters: 400, epoch: 8 | loss: 0.4923600
	speed: 0.2146s/iter; left time: 5018.1406s
	iters: 500, epoch: 8 | loss: 0.6034741
	speed: 0.2083s/iter; left time: 4849.3397s
Epoch: 8 cost time: 110.26054859161377
Epoch: 8, Steps: 553 | Train Loss: 0.5621249 Vali Loss: 0.5968724 Test Loss: 0.3097864
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.6019118
	speed: 1.0467s/iter; left time: 24207.1741s
	iters: 200, epoch: 9 | loss: 0.5688910
	speed: 0.2025s/iter; left time: 4663.5451s
	iters: 300, epoch: 9 | loss: 0.5684128
	speed: 0.1832s/iter; left time: 4200.0708s
	iters: 400, epoch: 9 | loss: 0.4392715
	speed: 0.1979s/iter; left time: 4517.9160s
	iters: 500, epoch: 9 | loss: 0.4993702
	speed: 0.2246s/iter; left time: 5103.7242s
Epoch: 9 cost time: 115.2055869102478
Epoch: 9, Steps: 553 | Train Loss: 0.5617026 Vali Loss: 0.5958897 Test Loss: 0.3096546
Validation loss decreased (0.596398 --> 0.595890).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.6756970
	speed: 1.0342s/iter; left time: 23346.9863s
	iters: 200, epoch: 10 | loss: 0.5458773
	speed: 0.2047s/iter; left time: 4601.2347s
	iters: 300, epoch: 10 | loss: 0.4955173
	speed: 0.2028s/iter; left time: 4536.8008s
	iters: 400, epoch: 10 | loss: 0.5104192
	speed: 0.2020s/iter; left time: 4499.9751s
	iters: 500, epoch: 10 | loss: 0.7263004
	speed: 0.1979s/iter; left time: 4388.5778s
Epoch: 10 cost time: 113.2087516784668
Epoch: 10, Steps: 553 | Train Loss: 0.5614040 Vali Loss: 0.5947301 Test Loss: 0.3093076
Validation loss decreased (0.595890 --> 0.594730).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5377880
	speed: 0.9187s/iter; left time: 20230.4691s
	iters: 200, epoch: 11 | loss: 0.6465798
	speed: 0.1929s/iter; left time: 4228.7002s
	iters: 300, epoch: 11 | loss: 0.5264456
	speed: 0.1702s/iter; left time: 3713.4370s
	iters: 400, epoch: 11 | loss: 0.4882153
	speed: 0.2111s/iter; left time: 4584.8823s
	iters: 500, epoch: 11 | loss: 0.5043219
	speed: 0.2233s/iter; left time: 4826.9190s
Epoch: 11 cost time: 110.57451224327087
Epoch: 11, Steps: 553 | Train Loss: 0.5611133 Vali Loss: 0.5952423 Test Loss: 0.3091213
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6231477
	speed: 1.0599s/iter; left time: 22753.7207s
	iters: 200, epoch: 12 | loss: 0.6139098
	speed: 0.2180s/iter; left time: 4657.3400s
	iters: 300, epoch: 12 | loss: 0.7363285
	speed: 0.1790s/iter; left time: 3807.2466s
	iters: 400, epoch: 12 | loss: 0.6628449
	speed: 0.1601s/iter; left time: 3389.6927s
	iters: 500, epoch: 12 | loss: 0.6555755
	speed: 0.2168s/iter; left time: 4568.5197s
Epoch: 12 cost time: 111.10588479042053
Epoch: 12, Steps: 553 | Train Loss: 0.5610648 Vali Loss: 0.5944011 Test Loss: 0.3090658
Validation loss decreased (0.594730 --> 0.594401).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.7193362
	speed: 1.0083s/iter; left time: 21088.2805s
	iters: 200, epoch: 13 | loss: 0.4743749
	speed: 0.2273s/iter; left time: 4730.3858s
	iters: 300, epoch: 13 | loss: 0.6139078
	speed: 0.2183s/iter; left time: 4521.1523s
	iters: 400, epoch: 13 | loss: 0.5138223
	speed: 0.2333s/iter; left time: 4808.7721s
	iters: 500, epoch: 13 | loss: 0.7062979
	speed: 0.2315s/iter; left time: 4749.8635s
Epoch: 13 cost time: 127.09962558746338
Epoch: 13, Steps: 553 | Train Loss: 0.5607170 Vali Loss: 0.5941437 Test Loss: 0.3089672
Validation loss decreased (0.594401 --> 0.594144).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5342926
	speed: 1.0220s/iter; left time: 20809.8469s
	iters: 200, epoch: 14 | loss: 0.5230553
	speed: 0.1876s/iter; left time: 3802.0763s
	iters: 300, epoch: 14 | loss: 0.6274839
	speed: 0.2101s/iter; left time: 4235.6664s
	iters: 400, epoch: 14 | loss: 0.5518452
	speed: 0.2021s/iter; left time: 4053.8678s
	iters: 500, epoch: 14 | loss: 0.6572073
	speed: 0.1750s/iter; left time: 3492.4297s
Epoch: 14 cost time: 106.13394093513489
Epoch: 14, Steps: 553 | Train Loss: 0.5606153 Vali Loss: 0.5941852 Test Loss: 0.3088325
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4958985
	speed: 1.0072s/iter; left time: 19950.8511s
	iters: 200, epoch: 15 | loss: 0.7153580
	speed: 0.2125s/iter; left time: 4188.3900s
	iters: 300, epoch: 15 | loss: 0.5916940
	speed: 0.2180s/iter; left time: 4275.3416s
	iters: 400, epoch: 15 | loss: 0.5050627
	speed: 0.2273s/iter; left time: 4435.2196s
	iters: 500, epoch: 15 | loss: 0.5487574
	speed: 0.2224s/iter; left time: 4315.6115s
Epoch: 15 cost time: 122.54201936721802
Epoch: 15, Steps: 553 | Train Loss: 0.5603227 Vali Loss: 0.5945138 Test Loss: 0.3088368
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4305752
	speed: 1.0611s/iter; left time: 20433.0758s
	iters: 200, epoch: 16 | loss: 0.5841796
	speed: 0.2021s/iter; left time: 3872.0085s
	iters: 300, epoch: 16 | loss: 0.5773864
	speed: 0.1818s/iter; left time: 3464.1233s
	iters: 400, epoch: 16 | loss: 0.5737438
	speed: 0.1773s/iter; left time: 3361.4749s
	iters: 500, epoch: 16 | loss: 0.5095451
	speed: 0.1965s/iter; left time: 3705.3908s
Epoch: 16 cost time: 109.0752534866333
Epoch: 16, Steps: 553 | Train Loss: 0.5603230 Vali Loss: 0.5934553 Test Loss: 0.3085814
Validation loss decreased (0.594144 --> 0.593455).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5431986
	speed: 0.8886s/iter; left time: 16619.2096s
	iters: 200, epoch: 17 | loss: 0.4734311
	speed: 0.1573s/iter; left time: 2926.7325s
	iters: 300, epoch: 17 | loss: 0.5020053
	speed: 0.1603s/iter; left time: 2965.2500s
	iters: 400, epoch: 17 | loss: 0.5528862
	speed: 0.1776s/iter; left time: 3269.0285s
	iters: 500, epoch: 17 | loss: 0.6454817
	speed: 0.1769s/iter; left time: 3237.6179s
Epoch: 17 cost time: 93.12124943733215
Epoch: 17, Steps: 553 | Train Loss: 0.5602321 Vali Loss: 0.5935405 Test Loss: 0.3086581
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5114928
	speed: 0.7803s/iter; left time: 14163.3396s
	iters: 200, epoch: 18 | loss: 0.7335405
	speed: 0.1585s/iter; left time: 2861.6981s
	iters: 300, epoch: 18 | loss: 0.4923753
	speed: 0.1549s/iter; left time: 2779.9139s
	iters: 400, epoch: 18 | loss: 0.5703895
	speed: 0.1124s/iter; left time: 2007.2034s
	iters: 500, epoch: 18 | loss: 0.5243578
	speed: 0.1268s/iter; left time: 2249.8515s
Epoch: 18 cost time: 75.3817389011383
Epoch: 18, Steps: 553 | Train Loss: 0.5596100 Vali Loss: 0.5936954 Test Loss: 0.3086908
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4821431
	speed: 0.6512s/iter; left time: 11458.8595s
	iters: 200, epoch: 19 | loss: 0.5060256
	speed: 0.1563s/iter; left time: 2734.1502s
	iters: 300, epoch: 19 | loss: 0.4515913
	speed: 0.1566s/iter; left time: 2724.8965s
	iters: 400, epoch: 19 | loss: 0.4385141
	speed: 0.1573s/iter; left time: 2721.1090s
	iters: 500, epoch: 19 | loss: 0.7441760
	speed: 0.1592s/iter; left time: 2738.1347s
Epoch: 19 cost time: 88.41824388504028
Epoch: 19, Steps: 553 | Train Loss: 0.5598855 Vali Loss: 0.5934761 Test Loss: 0.3085765
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.30787578225135803, mae:0.32993122935295105, rse:0.7301629781723022, corr:[0.4739543  0.4747084  0.47406736 0.47330943 0.4726155  0.4719627
 0.4712698  0.47042206 0.46935526 0.46810198 0.46685585 0.46590388
 0.4652728  0.4649284  0.4647198  0.46439716 0.46386242 0.46300352
 0.46191823 0.46072233 0.45961455 0.4587311  0.45818588 0.45782042
 0.45752963 0.45713344 0.45650896 0.45561418 0.4545489  0.45342094
 0.45242146 0.4516151  0.45105463 0.45060676 0.45020422 0.44968814
 0.44910336 0.44839472 0.44761944 0.44688582 0.4463199  0.44600815
 0.44585058 0.44566548 0.44532266 0.44475263 0.44400406 0.44315228
 0.442139   0.44118145 0.44043398 0.43987587 0.43953398 0.4393067
 0.43908027 0.43873665 0.4382559  0.43762252 0.43693522 0.43624046
 0.43564954 0.43520853 0.4349532  0.434819   0.43474272 0.43467584
 0.43453878 0.4343247  0.43406215 0.43378216 0.43355682 0.43330455
 0.4331257  0.4329885  0.43282595 0.43262398 0.4323941  0.43217608
 0.43194202 0.43166447 0.43139857 0.4311161  0.4307981  0.43049797
 0.43026292 0.43008196 0.42994353 0.42973578 0.42953128 0.42928913
 0.4290586  0.42886576 0.42866924 0.4285036  0.428377   0.42824996
 0.4281103  0.42794377 0.42773718 0.42753547 0.42734453 0.42717475
 0.42703742 0.4269224  0.4267952  0.4266292  0.42640254 0.42614833
 0.42582896 0.42549857 0.42515406 0.42476016 0.42432982 0.42394957
 0.42361757 0.42332235 0.42311195 0.42299548 0.42293343 0.42284915
 0.42276043 0.4226535  0.42248407 0.42227104 0.42199543 0.4216758
 0.42133594 0.4209716  0.42063513 0.42033    0.42002562 0.41973367
 0.41947064 0.41922817 0.4190283  0.4188365  0.41865775 0.4185045
 0.4182763  0.4180787  0.4179295  0.41783845 0.41771606 0.4175293
 0.41727605 0.41696343 0.41662544 0.4162853  0.41588175 0.41534865
 0.41470683 0.41403463 0.41333163 0.4125976  0.4119356  0.41133076
 0.41087994 0.41046244 0.41014507 0.40992683 0.40981722 0.40975434
 0.40964988 0.40947098 0.40914547 0.40870342 0.40811932 0.40746036
 0.406815   0.40621853 0.40568072 0.405215   0.4048078  0.404435
 0.40402117 0.4035495  0.40300825 0.4024449  0.4018913  0.4013541
 0.40089935 0.40048346 0.4000709  0.3996397  0.399208   0.39874756
 0.39825478 0.39769244 0.39710346 0.39652532 0.3959622  0.39543867
 0.39499965 0.39454642 0.39414468 0.39375475 0.39336005 0.3929741
 0.3925776  0.39220327 0.39190868 0.39162213 0.39132977 0.3909713
 0.39058107 0.39013734 0.38964126 0.38911995 0.38853067 0.38795123
 0.38738653 0.38686663 0.38638863 0.3859719  0.3855862  0.38518408
 0.38472834 0.38432992 0.38391924 0.38348407 0.3830907  0.3828217
 0.3826356  0.38246918 0.3822918  0.38205576 0.38174635 0.38140482
 0.38100713 0.38058308 0.3801342  0.3796447  0.37912852 0.3785898
 0.3780853  0.3776405  0.3771939  0.37676737 0.37631947 0.37590805
 0.37559196 0.3753765  0.37514734 0.37493902 0.37474215 0.37450102
 0.37417075 0.37379703 0.37337297 0.3729404  0.37251914 0.37214935
 0.37179878 0.37143746 0.37111846 0.37073234 0.37032768 0.36991084
 0.36954042 0.36920658 0.3688407  0.36853087 0.36824757 0.36806902
 0.36796194 0.3678841  0.3677924  0.36766964 0.36750835 0.36725482
 0.36694214 0.36664784 0.36634272 0.3659983  0.3656997  0.3654454
 0.36520648 0.36490497 0.36457634 0.36418608 0.36370948 0.36320448
 0.36271584 0.36217746 0.3616491  0.36112335 0.36059454 0.36002848
 0.35946184 0.35889032 0.3582965  0.35765475 0.35706523 0.35654435
 0.35601783 0.35549617 0.3549562  0.35438982 0.35373607 0.3530753
 0.35229158 0.35146964 0.35063568 0.34985942 0.349155   0.34851542
 0.3479424  0.34740224 0.34686598 0.34627455 0.34569305 0.34508705
 0.3444874  0.34396893 0.34350044 0.34306243 0.34266198 0.34224468
 0.3417945  0.34125465 0.3406276  0.3399581  0.3393122  0.3386748
 0.3381169  0.33765858 0.33723414 0.33687598 0.33659294 0.33629778
 0.33600077 0.33566827 0.3353159  0.3350171  0.33472973 0.33449554
 0.3342607  0.33400887 0.3337388  0.33338824 0.33294207 0.33246478
 0.3319871  0.33149406 0.3310422  0.33066273 0.3303829  0.3301632
 0.3300016  0.3298333  0.3296404  0.32937315 0.32906592 0.32870895
 0.32833007 0.3279793  0.3277039  0.32746655 0.3272773  0.32713836
 0.3269873  0.32681796 0.32664204 0.32639605 0.32609883 0.32577044
 0.3254133  0.32507095 0.32477748 0.32448897 0.32422808 0.3239847
 0.32376465 0.32353693 0.32331184 0.32310542 0.32290003 0.32267386
 0.3224218  0.32216805 0.32187644 0.32156524 0.32127002 0.32092193
 0.32056278 0.32020923 0.31990838 0.31964287 0.31942543 0.3192511
 0.31905842 0.31891602 0.31877562 0.31865034 0.31853455 0.31842706
 0.31831798 0.31820625 0.31805936 0.31787005 0.31764492 0.31736758
 0.31710672 0.31682295 0.3165655  0.31631425 0.31606632 0.31585774
 0.31570578 0.3155771  0.31545275 0.31526166 0.3150508  0.31484634
 0.31465492 0.31447852 0.31435865 0.31432468 0.3142931  0.31426805
 0.31417063 0.31404495 0.3138058  0.31350508 0.31310958 0.31264082
 0.3121418  0.31168693 0.31123772 0.31076    0.31026772 0.30982405
 0.30939102 0.30898532 0.30865246 0.30838755 0.30815583 0.30792972
 0.30764577 0.30727804 0.3068195  0.30621308 0.30546612 0.30447245
 0.3033864  0.30227444 0.3012054  0.30031383 0.29958263 0.2990361
 0.29864302 0.2982899  0.29793024 0.29751205 0.29705194 0.29650912
 0.2959443  0.29539925 0.2949056  0.29445967 0.29403633 0.29367772
 0.29333058 0.29299167 0.2926738  0.29236615 0.2920684  0.29176885
 0.29149657 0.2912423  0.29097146 0.29069248 0.29044613 0.29021773
 0.2900195  0.28986645 0.28975102 0.28968555 0.28966558 0.2896387
 0.2896222  0.2895665  0.28941965 0.28917593 0.28885084 0.28849342
 0.28807566 0.2876623  0.2873071  0.28702438 0.28682253 0.28665754
 0.2865307  0.28641868 0.28625903 0.2860411  0.2857539  0.28544566
 0.28514752 0.2848918  0.2846694  0.2844606  0.2842905  0.28412881
 0.28392884 0.28368813 0.2834408  0.2831123  0.2827858  0.2824506
 0.28216788 0.28193995 0.28177506 0.28161025 0.28144065 0.28126994
 0.28108242 0.2808854  0.2806674  0.2804615  0.28026673 0.28006083
 0.27986607 0.27965918 0.27940768 0.27914327 0.27882302 0.27846816
 0.27806157 0.2776265  0.2771896  0.2767755  0.27644363 0.27615178
 0.27591202 0.27570963 0.27550638 0.2752939  0.27505088 0.27475104
 0.27439374 0.27402243 0.27360234 0.27318627 0.2728016  0.27243635
 0.27211675 0.271867   0.27168086 0.27149966 0.2713102  0.27104086
 0.27068597 0.2702469  0.26976317 0.269223   0.26870754 0.2682109
 0.2677781  0.2673865  0.26704392 0.2667467  0.26641107 0.26601633
 0.265573   0.26510394 0.26462954 0.2641769  0.26377296 0.26347083
 0.26325503 0.2631124  0.26300785 0.26286495 0.2626075  0.26215392
 0.26173267 0.26102152 0.26015064 0.25952435 0.25887123 0.25824136
 0.25760514 0.2569542  0.25626087 0.25557312 0.2548781  0.25425607
 0.25373828 0.25334767 0.25308353 0.25295532 0.25290853 0.25290775
 0.2528637  0.25272027 0.25246504 0.25209734 0.251639   0.25111628
 0.25057656 0.25000843 0.24943669 0.2488571  0.24828237 0.24774425
 0.24719542 0.24669875 0.24621236 0.2457507  0.24536026 0.24503385
 0.24477224 0.24458933 0.2444119  0.24421225 0.24398054 0.24362355
 0.24319847 0.24277176 0.24233052 0.24190475 0.24153604 0.24126437
 0.24100469 0.24076034 0.24049222 0.24017935 0.23988728 0.23957138
 0.23929591 0.2390757  0.23898016 0.23899473 0.23911594 0.2393305
 0.23959202 0.23982058 0.23993781 0.23988178 0.23968795 0.23936635
 0.23900323 0.2386521  0.23836294 0.23812741 0.23801874 0.23801726
 0.2381236  0.23822042 0.2383163  0.23827995 0.23816842 0.23792815
 0.23762234 0.237257   0.23688494 0.23655279 0.23631814 0.23622523
 0.23628984 0.2364592  0.23667493 0.23686136 0.2369951  0.2370335
 0.2369399  0.23671241 0.23633377 0.2358705  0.2353758  0.23484573
 0.23437087 0.2339681  0.23359759 0.23325828 0.23291606 0.23258741
 0.23226541 0.23198259 0.23178688 0.23165059 0.23160213 0.23157308
 0.23151676 0.23140466 0.23125532 0.23103824 0.230757   0.23043153
 0.23008369 0.2297018  0.22938204 0.2291523  0.22899078 0.22886918
 0.22879648 0.22873749 0.22866973 0.22857277 0.22843985 0.22820693
 0.22790359 0.22753185 0.22708529 0.22661617 0.22619174 0.22590292
 0.22577825 0.22581808 0.2259906  0.2262333  0.22631036 0.22618671
 0.22578833 0.22543308 0.2251844  0.22521912 0.22535095 0.22488557]
