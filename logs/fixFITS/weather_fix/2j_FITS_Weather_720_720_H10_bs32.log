Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=70, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=3, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:3
>>>>>>>start training : Weather_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=70, out_features=140, bias=True)
    (1): Linear(in_features=70, out_features=140, bias=True)
    (2): Linear(in_features=70, out_features=140, bias=True)
    (3): Linear(in_features=70, out_features=140, bias=True)
    (4): Linear(in_features=70, out_features=140, bias=True)
    (5): Linear(in_features=70, out_features=140, bias=True)
    (6): Linear(in_features=70, out_features=140, bias=True)
    (7): Linear(in_features=70, out_features=140, bias=True)
    (8): Linear(in_features=70, out_features=140, bias=True)
    (9): Linear(in_features=70, out_features=140, bias=True)
    (10): Linear(in_features=70, out_features=140, bias=True)
    (11): Linear(in_features=70, out_features=140, bias=True)
    (12): Linear(in_features=70, out_features=140, bias=True)
    (13): Linear(in_features=70, out_features=140, bias=True)
    (14): Linear(in_features=70, out_features=140, bias=True)
    (15): Linear(in_features=70, out_features=140, bias=True)
    (16): Linear(in_features=70, out_features=140, bias=True)
    (17): Linear(in_features=70, out_features=140, bias=True)
    (18): Linear(in_features=70, out_features=140, bias=True)
    (19): Linear(in_features=70, out_features=140, bias=True)
    (20): Linear(in_features=70, out_features=140, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  13171200.0
params:  208740.0
Trainable parameters:  208740
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7202252
	speed: 0.1599s/iter; left time: 4404.8696s
	iters: 200, epoch: 1 | loss: 0.5931120
	speed: 0.1517s/iter; left time: 4163.7923s
	iters: 300, epoch: 1 | loss: 0.5447056
	speed: 0.1550s/iter; left time: 4239.2540s
	iters: 400, epoch: 1 | loss: 0.4701969
	speed: 0.1514s/iter; left time: 4126.2651s
	iters: 500, epoch: 1 | loss: 0.4712469
	speed: 0.1617s/iter; left time: 4390.5575s
Epoch: 1 cost time: 86.96566557884216
Epoch: 1, Steps: 553 | Train Loss: 0.6188832 Vali Loss: 0.6841549 Test Loss: 0.3354676
Validation loss decreased (inf --> 0.684155).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5330241
	speed: 0.6989s/iter; left time: 18869.6365s
	iters: 200, epoch: 2 | loss: 0.4240173
	speed: 0.1166s/iter; left time: 3135.6453s
	iters: 300, epoch: 2 | loss: 0.4502455
	speed: 0.1526s/iter; left time: 4089.0534s
	iters: 400, epoch: 2 | loss: 0.4106563
	speed: 0.1507s/iter; left time: 4022.1736s
	iters: 500, epoch: 2 | loss: 0.4453436
	speed: 0.1599s/iter; left time: 4252.0427s
Epoch: 2 cost time: 77.52078056335449
Epoch: 2, Steps: 553 | Train Loss: 0.4293247 Vali Loss: 0.6300154 Test Loss: 0.3213295
Validation loss decreased (0.684155 --> 0.630015).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3722277
	speed: 0.8108s/iter; left time: 21441.3663s
	iters: 200, epoch: 3 | loss: 0.3702281
	speed: 0.1635s/iter; left time: 4307.5563s
	iters: 300, epoch: 3 | loss: 0.3536744
	speed: 0.1717s/iter; left time: 4506.9369s
	iters: 400, epoch: 3 | loss: 0.3943011
	speed: 0.1611s/iter; left time: 4211.9154s
	iters: 500, epoch: 3 | loss: 0.4013908
	speed: 0.1141s/iter; left time: 2972.9630s
Epoch: 3 cost time: 86.76012539863586
Epoch: 3, Steps: 553 | Train Loss: 0.3743442 Vali Loss: 0.6074720 Test Loss: 0.3154044
Validation loss decreased (0.630015 --> 0.607472).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2762245
	speed: 0.7073s/iter; left time: 18312.8118s
	iters: 200, epoch: 4 | loss: 0.3922020
	speed: 0.1531s/iter; left time: 3947.6320s
	iters: 300, epoch: 4 | loss: 0.3419172
	speed: 0.1519s/iter; left time: 3901.3853s
	iters: 400, epoch: 4 | loss: 0.3661138
	speed: 0.1409s/iter; left time: 3605.8641s
	iters: 500, epoch: 4 | loss: 0.2491679
	speed: 0.1426s/iter; left time: 3635.1288s
Epoch: 4 cost time: 83.3132472038269
Epoch: 4, Steps: 553 | Train Loss: 0.3509354 Vali Loss: 0.5990517 Test Loss: 0.3130730
Validation loss decreased (0.607472 --> 0.599052).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2937637
	speed: 0.6465s/iter; left time: 16381.8260s
	iters: 200, epoch: 5 | loss: 0.4270345
	speed: 0.1289s/iter; left time: 3254.3360s
	iters: 300, epoch: 5 | loss: 0.3291247
	speed: 0.1434s/iter; left time: 3605.8247s
	iters: 400, epoch: 5 | loss: 0.2904537
	speed: 0.1323s/iter; left time: 3313.2668s
	iters: 500, epoch: 5 | loss: 0.3530932
	speed: 0.1426s/iter; left time: 3555.6403s
Epoch: 5 cost time: 76.90818572044373
Epoch: 5, Steps: 553 | Train Loss: 0.3406932 Vali Loss: 0.5970029 Test Loss: 0.3119583
Validation loss decreased (0.599052 --> 0.597003).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3283975
	speed: 0.7270s/iter; left time: 18019.5375s
	iters: 200, epoch: 6 | loss: 0.3146659
	speed: 0.1546s/iter; left time: 3816.6570s
	iters: 300, epoch: 6 | loss: 0.3002512
	speed: 0.1580s/iter; left time: 3883.3905s
	iters: 400, epoch: 6 | loss: 0.3331681
	speed: 0.1585s/iter; left time: 3880.7323s
	iters: 500, epoch: 6 | loss: 0.3336187
	speed: 0.1629s/iter; left time: 3973.5837s
Epoch: 6 cost time: 89.92433667182922
Epoch: 6, Steps: 553 | Train Loss: 0.3364299 Vali Loss: 0.5967128 Test Loss: 0.3114644
Validation loss decreased (0.597003 --> 0.596713).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2986621
	speed: 0.6945s/iter; left time: 16828.6242s
	iters: 200, epoch: 7 | loss: 0.3445279
	speed: 0.1366s/iter; left time: 3296.6502s
	iters: 300, epoch: 7 | loss: 0.2910188
	speed: 0.1419s/iter; left time: 3410.3640s
	iters: 400, epoch: 7 | loss: 0.3354794
	speed: 0.1290s/iter; left time: 3086.4280s
	iters: 500, epoch: 7 | loss: 0.3329073
	speed: 0.1362s/iter; left time: 3245.5110s
Epoch: 7 cost time: 75.49922752380371
Epoch: 7, Steps: 553 | Train Loss: 0.3348029 Vali Loss: 0.5981883 Test Loss: 0.3114714
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3098997
	speed: 0.8129s/iter; left time: 19250.3279s
	iters: 200, epoch: 8 | loss: 0.3922050
	speed: 0.1547s/iter; left time: 3647.0147s
	iters: 300, epoch: 8 | loss: 0.3074561
	speed: 0.1673s/iter; left time: 3928.3658s
	iters: 400, epoch: 8 | loss: 0.2968425
	speed: 0.1566s/iter; left time: 3661.6667s
	iters: 500, epoch: 8 | loss: 0.3337803
	speed: 0.1721s/iter; left time: 4005.8891s
Epoch: 8 cost time: 93.05338978767395
Epoch: 8, Steps: 553 | Train Loss: 0.3344999 Vali Loss: 0.5992913 Test Loss: 0.3112700
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3724919
	speed: 0.7736s/iter; left time: 17891.2857s
	iters: 200, epoch: 9 | loss: 0.3443364
	speed: 0.1615s/iter; left time: 3718.7141s
	iters: 300, epoch: 9 | loss: 0.3545004
	speed: 0.1690s/iter; left time: 3874.4205s
	iters: 400, epoch: 9 | loss: 0.2519748
	speed: 0.1922s/iter; left time: 4387.9181s
	iters: 500, epoch: 9 | loss: 0.2783205
	speed: 0.1885s/iter; left time: 4283.2967s
Epoch: 9 cost time: 96.44382524490356
Epoch: 9, Steps: 553 | Train Loss: 0.3343014 Vali Loss: 0.5988134 Test Loss: 0.3112076
EarlyStopping counter: 3 out of 3
Early stopping
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=70, out_features=140, bias=True)
    (1): Linear(in_features=70, out_features=140, bias=True)
    (2): Linear(in_features=70, out_features=140, bias=True)
    (3): Linear(in_features=70, out_features=140, bias=True)
    (4): Linear(in_features=70, out_features=140, bias=True)
    (5): Linear(in_features=70, out_features=140, bias=True)
    (6): Linear(in_features=70, out_features=140, bias=True)
    (7): Linear(in_features=70, out_features=140, bias=True)
    (8): Linear(in_features=70, out_features=140, bias=True)
    (9): Linear(in_features=70, out_features=140, bias=True)
    (10): Linear(in_features=70, out_features=140, bias=True)
    (11): Linear(in_features=70, out_features=140, bias=True)
    (12): Linear(in_features=70, out_features=140, bias=True)
    (13): Linear(in_features=70, out_features=140, bias=True)
    (14): Linear(in_features=70, out_features=140, bias=True)
    (15): Linear(in_features=70, out_features=140, bias=True)
    (16): Linear(in_features=70, out_features=140, bias=True)
    (17): Linear(in_features=70, out_features=140, bias=True)
    (18): Linear(in_features=70, out_features=140, bias=True)
    (19): Linear(in_features=70, out_features=140, bias=True)
    (20): Linear(in_features=70, out_features=140, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  13171200.0
params:  208740.0
Trainable parameters:  208740
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6273547
	speed: 0.1791s/iter; left time: 4935.6060s
	iters: 200, epoch: 1 | loss: 0.4728915
	speed: 0.1700s/iter; left time: 4666.3017s
	iters: 300, epoch: 1 | loss: 0.5305561
	speed: 0.1583s/iter; left time: 4330.2506s
	iters: 400, epoch: 1 | loss: 0.5131491
	speed: 0.1375s/iter; left time: 3747.6668s
	iters: 500, epoch: 1 | loss: 0.4802476
	speed: 0.1564s/iter; left time: 4245.0954s
Epoch: 1 cost time: 88.01815223693848
Epoch: 1, Steps: 553 | Train Loss: 0.5637794 Vali Loss: 0.5974501 Test Loss: 0.3104081
Validation loss decreased (inf --> 0.597450).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4879446
	speed: 0.7208s/iter; left time: 19461.1300s
	iters: 200, epoch: 2 | loss: 0.5826015
	speed: 0.1384s/iter; left time: 3722.7009s
	iters: 300, epoch: 2 | loss: 0.5232917
	speed: 0.1340s/iter; left time: 3591.2510s
	iters: 400, epoch: 2 | loss: 0.5583926
	speed: 0.1649s/iter; left time: 4402.5190s
	iters: 500, epoch: 2 | loss: 0.5032068
	speed: 0.1582s/iter; left time: 4208.6479s
Epoch: 2 cost time: 84.30797410011292
Epoch: 2, Steps: 553 | Train Loss: 0.5624277 Vali Loss: 0.5966671 Test Loss: 0.3099993
Validation loss decreased (0.597450 --> 0.596667).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4347653
	speed: 0.7890s/iter; left time: 20863.9646s
	iters: 200, epoch: 3 | loss: 0.5899773
	speed: 0.1534s/iter; left time: 4041.2526s
	iters: 300, epoch: 3 | loss: 0.5813743
	speed: 0.1461s/iter; left time: 3833.9936s
	iters: 400, epoch: 3 | loss: 0.6282897
	speed: 0.1686s/iter; left time: 4406.8566s
	iters: 500, epoch: 3 | loss: 0.4874299
	speed: 0.1750s/iter; left time: 4557.4032s
Epoch: 3 cost time: 90.99078130722046
Epoch: 3, Steps: 553 | Train Loss: 0.5620434 Vali Loss: 0.5944119 Test Loss: 0.3097011
Validation loss decreased (0.596667 --> 0.594412).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5646625
	speed: 0.8321s/iter; left time: 21544.2218s
	iters: 200, epoch: 4 | loss: 0.5289684
	speed: 0.1718s/iter; left time: 4431.6493s
	iters: 300, epoch: 4 | loss: 0.4748955
	speed: 0.1902s/iter; left time: 4885.4281s
	iters: 400, epoch: 4 | loss: 0.5177541
	speed: 0.1772s/iter; left time: 4535.7106s
	iters: 500, epoch: 4 | loss: 0.4748061
	speed: 0.1743s/iter; left time: 4444.5042s
Epoch: 4 cost time: 98.88309669494629
Epoch: 4, Steps: 553 | Train Loss: 0.5615304 Vali Loss: 0.5952521 Test Loss: 0.3095611
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5594353
	speed: 0.8194s/iter; left time: 20763.2271s
	iters: 200, epoch: 5 | loss: 0.5337292
	speed: 0.1537s/iter; left time: 3878.1547s
	iters: 300, epoch: 5 | loss: 0.5224032
	speed: 0.1579s/iter; left time: 3969.4898s
	iters: 400, epoch: 5 | loss: 0.5951176
	speed: 0.1586s/iter; left time: 3972.0030s
	iters: 500, epoch: 5 | loss: 0.4914561
	speed: 0.1552s/iter; left time: 3869.4422s
Epoch: 5 cost time: 87.7039806842804
Epoch: 5, Steps: 553 | Train Loss: 0.5611635 Vali Loss: 0.5942202 Test Loss: 0.3088672
Validation loss decreased (0.594412 --> 0.594220).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6256269
	speed: 0.7799s/iter; left time: 19329.4363s
	iters: 200, epoch: 6 | loss: 0.5356037
	speed: 0.1522s/iter; left time: 3757.0691s
	iters: 300, epoch: 6 | loss: 0.4653606
	speed: 0.1565s/iter; left time: 3846.4823s
	iters: 400, epoch: 6 | loss: 0.5137933
	speed: 0.1691s/iter; left time: 4141.1240s
	iters: 500, epoch: 6 | loss: 0.6666307
	speed: 0.1535s/iter; left time: 3744.2180s
Epoch: 6 cost time: 88.12508773803711
Epoch: 6, Steps: 553 | Train Loss: 0.5610208 Vali Loss: 0.5940511 Test Loss: 0.3086952
Validation loss decreased (0.594220 --> 0.594051).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5181037
	speed: 0.7714s/iter; left time: 18693.8182s
	iters: 200, epoch: 7 | loss: 0.4389466
	speed: 0.1613s/iter; left time: 3891.5719s
	iters: 300, epoch: 7 | loss: 0.6345263
	speed: 0.1601s/iter; left time: 3847.6012s
	iters: 400, epoch: 7 | loss: 0.6347131
	speed: 0.1584s/iter; left time: 3790.0200s
	iters: 500, epoch: 7 | loss: 0.5339280
	speed: 0.1275s/iter; left time: 3039.2948s
Epoch: 7 cost time: 84.89611673355103
Epoch: 7, Steps: 553 | Train Loss: 0.5606775 Vali Loss: 0.5932879 Test Loss: 0.3086995
Validation loss decreased (0.594051 --> 0.593288).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.6546559
	speed: 0.7586s/iter; left time: 17963.9091s
	iters: 200, epoch: 8 | loss: 0.5420367
	speed: 0.1687s/iter; left time: 3978.6584s
	iters: 300, epoch: 8 | loss: 0.4927912
	speed: 0.1733s/iter; left time: 4069.8972s
	iters: 400, epoch: 8 | loss: 0.7580402
	speed: 0.1628s/iter; left time: 3805.0966s
	iters: 500, epoch: 8 | loss: 0.5524758
	speed: 0.1601s/iter; left time: 3726.3338s
Epoch: 8 cost time: 94.22887825965881
Epoch: 8, Steps: 553 | Train Loss: 0.5605840 Vali Loss: 0.5937430 Test Loss: 0.3085510
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5873172
	speed: 0.7560s/iter; left time: 17484.5709s
	iters: 200, epoch: 9 | loss: 0.5335498
	speed: 0.1617s/iter; left time: 3723.6093s
	iters: 300, epoch: 9 | loss: 0.5414394
	speed: 0.1568s/iter; left time: 3593.8253s
	iters: 400, epoch: 9 | loss: 0.4867936
	speed: 0.1503s/iter; left time: 3430.4970s
	iters: 500, epoch: 9 | loss: 0.5091698
	speed: 0.1687s/iter; left time: 3833.6539s
Epoch: 9 cost time: 89.93020153045654
Epoch: 9, Steps: 553 | Train Loss: 0.5603644 Vali Loss: 0.5940610 Test Loss: 0.3082516
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5066352
	speed: 0.8451s/iter; left time: 19078.3070s
	iters: 200, epoch: 10 | loss: 0.4596452
	speed: 0.1817s/iter; left time: 4084.1102s
	iters: 300, epoch: 10 | loss: 0.5189863
	speed: 0.1764s/iter; left time: 3946.5995s
	iters: 400, epoch: 10 | loss: 0.5611688
	speed: 0.1664s/iter; left time: 3707.1709s
	iters: 500, epoch: 10 | loss: 0.4498945
	speed: 0.1506s/iter; left time: 3339.1575s
Epoch: 10 cost time: 94.17326140403748
Epoch: 10, Steps: 553 | Train Loss: 0.5602675 Vali Loss: 0.5933012 Test Loss: 0.3081611
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.3079966604709625, mae:0.33013805747032166, rse:0.7303063273429871, corr:[0.46968055 0.472566   0.47260454 0.47148758 0.47013542 0.4691622
 0.46878353 0.468805   0.46886104 0.4685788  0.46781003 0.46673068
 0.4654962  0.464383   0.4635926  0.4631381  0.462962   0.46278945
 0.46242973 0.4617264  0.460719   0.459529   0.45834953 0.4571555
 0.45599943 0.45485246 0.45368582 0.45247585 0.4513155  0.45031074
 0.44963706 0.44933143 0.449392   0.44958183 0.44974995 0.4496907
 0.4494467  0.44895953 0.44826868 0.44746816 0.44669902 0.4461075
 0.44567767 0.4453193  0.44495282 0.4444827  0.44390044 0.44322273
 0.44234717 0.4414771  0.44075897 0.44018936 0.439812   0.4395395
 0.43926525 0.43887237 0.4383599  0.43771124 0.4370244  0.43633878
 0.43576595 0.435352   0.4351369  0.43504986 0.43501043 0.43495318
 0.4347837  0.43449053 0.4341108  0.43368986 0.433322   0.43295115
 0.432699   0.4325408  0.432405   0.43225056 0.43205553 0.43184343
 0.43158343 0.4312636  0.43095413 0.4306503  0.43037754 0.43018898
 0.43010628 0.43008575 0.43008742 0.4299706  0.42982042 0.4296069
 0.42939827 0.42923275 0.42906106 0.42891365 0.42878607 0.42864162
 0.42846504 0.42824936 0.42798907 0.42773357 0.42749712 0.42730474
 0.42718834 0.4271237  0.42706648 0.42697063 0.42677853 0.4265171
 0.42613193 0.42569405 0.4252125  0.42468673 0.4241528  0.42372015
 0.42339274 0.42315295 0.42303458 0.42303047 0.4230777  0.42306685
 0.42302367 0.42293623 0.42276156 0.42252204 0.42220503 0.42182705
 0.42139685 0.42092228 0.42047283 0.4200795  0.4197481  0.4195069
 0.41937315 0.41930804 0.41928932 0.41923255 0.41910952 0.4189112
 0.4185345  0.4181238  0.4177403  0.41743308 0.41715097 0.41687816
 0.41660652 0.4163288  0.41604826 0.41576618 0.41541535 0.41493267
 0.41435328 0.4137922  0.41326162 0.4127602  0.41236222 0.41200313
 0.4116809  0.4112185  0.41066518 0.41006246 0.40949044 0.40899494
 0.40859058 0.40830302 0.40806204 0.40784067 0.40752766 0.40709746
 0.4065572  0.40593317 0.40524417 0.40456173 0.40395644 0.40349224
 0.4031187  0.4028227  0.40253484 0.40222877 0.4018534  0.4013731
 0.40085724 0.40029338 0.39971283 0.3991518  0.398674   0.39826903
 0.39791727 0.39753926 0.3971272  0.39665854 0.39612335 0.3955573
 0.39504534 0.39453143 0.3940932  0.39371708 0.39337838 0.3930584
 0.39269644 0.39228374 0.3918632  0.3913726  0.3908363  0.3902598
 0.38975078 0.3893287  0.38899788 0.38874292 0.38845396 0.38813564
 0.3877299  0.3872348  0.386659   0.38606933 0.38549957 0.38496426
 0.38445732 0.3840959  0.3837702  0.38341725 0.38303608 0.3826681
 0.38229123 0.38189697 0.38152543 0.3811932  0.38094023 0.38082427
 0.38079405 0.3808178  0.3808171  0.38068777 0.38036746 0.37981987
 0.37909415 0.37829453 0.37746578 0.3767166  0.37606677 0.37559554
 0.37535018 0.37527078 0.3751887  0.37509108 0.37493753 0.37466627
 0.37425423 0.37378496 0.37328276 0.3728226  0.37243494 0.37215772
 0.3719507  0.37172797 0.37151203 0.37115642 0.3707161  0.37022674
 0.36979792 0.36945975 0.36910364 0.36879325 0.36848685 0.36826897
 0.3680916  0.36790878 0.36769593 0.36745983 0.3671968  0.36686736
 0.36649588 0.36614603 0.36576638 0.36532456 0.36489645 0.36447948
 0.36405462 0.36355925 0.36305735 0.36254492 0.36202216 0.36155754
 0.36119398 0.36084926 0.36055613 0.3602764  0.35996738 0.35957336
 0.35912654 0.3586146  0.35800594 0.35727516 0.35655135 0.35587868
 0.35523906 0.35468006 0.35419467 0.35376543 0.35329646 0.3528258
 0.35220066 0.3514693  0.35061875 0.3497142  0.34879583 0.34791556
 0.34716105 0.34655264 0.3460912  0.34569088 0.34536955 0.3450274
 0.34463105 0.34421092 0.34372193 0.3431562  0.34255594 0.34191215
 0.34127325 0.3406238  0.33998695 0.33940357 0.33890143 0.33840618
 0.33795086 0.33753833 0.33710656 0.33670765 0.33637908 0.3360641
 0.3357845  0.33550426 0.3352186  0.33497334 0.33470726 0.33445194
 0.3341695  0.33387116 0.33359095 0.33328643 0.33294562 0.33261347
 0.33228004 0.33186612 0.33139554 0.33089367 0.33042002 0.32999453
 0.3296948  0.3295017  0.3294531  0.32948983 0.3295735  0.32962894
 0.32960296 0.32946625 0.3292215  0.3288265  0.32832342 0.32778838
 0.32724372 0.32677853 0.32649425 0.3263235  0.32626    0.32625517
 0.32623076 0.3261611  0.32603338 0.32579806 0.32549828 0.3251708
 0.32486713 0.3245926  0.3243758  0.32422355 0.3240958  0.32393223
 0.32369736 0.32339916 0.32299647 0.32252264 0.3220436  0.32151622
 0.32100528 0.3205358  0.32015753 0.31984168 0.31960425 0.3194345
 0.3192778  0.31921175 0.3191948  0.3192339  0.31930736 0.31938624
 0.31942242 0.31938466 0.31921858 0.3189143  0.3184841  0.31793758
 0.3173914  0.31684542 0.31639153 0.3160393  0.3157867  0.31563926
 0.3155918  0.31559548 0.3156014  0.31552714 0.31541592 0.31528988
 0.31514797 0.31498602 0.31484324 0.31474915 0.3146286  0.31449413
 0.31428787 0.31404287 0.31368378 0.31326285 0.31274202 0.31216326
 0.31157154 0.31106213 0.31060746 0.31018716 0.3098191  0.30956137
 0.30934286 0.30913275 0.3089313  0.3086911  0.30836138 0.3079335
 0.30738202 0.30674103 0.30606493 0.30534542 0.30460522 0.30374444
 0.3029082  0.30213806 0.30145347 0.3008983  0.300357   0.29981163
 0.29924026 0.29859427 0.29789656 0.29717064 0.29649252 0.2958585
 0.29535574 0.29498416 0.2947087  0.29445064 0.29412964 0.293769
 0.29333276 0.29285285 0.29238528 0.29195428 0.29158914 0.29128754
 0.29108027 0.29093236 0.2907767  0.29059973 0.29041752 0.2902011
 0.2899504  0.2896692  0.2893448  0.2889886  0.28861526 0.28820053
 0.2878172  0.28747165 0.28716302 0.2869302  0.2868015  0.28680173
 0.2868486  0.28693196 0.28702354 0.2870613  0.2870128  0.2868317
 0.2865536  0.28622207 0.28585118 0.28549984 0.28519717 0.28499654
 0.28490523 0.2849144  0.28496608 0.28499356 0.285002   0.28495622
 0.2848091  0.28457803 0.28432167 0.283972   0.28361365 0.2832417
 0.28291965 0.2826417  0.28242305 0.28222647 0.28206742 0.28194845
 0.2818553  0.2817686  0.28164005 0.28146464 0.28120783 0.280837
 0.28038022 0.27985716 0.2793041  0.278831   0.27845103 0.2782111
 0.27807778 0.27802196 0.27797303 0.27788374 0.27773747 0.27745554
 0.277062   0.27658707 0.27606577 0.27556276 0.27510652 0.2746975
 0.2743257  0.27399626 0.27362382 0.27320683 0.2727392  0.2722039
 0.2716576  0.27118784 0.2708611  0.27068025 0.27065766 0.27069363
 0.2707232  0.2706717  0.27050158 0.2701443  0.269653   0.26903665
 0.26838854 0.2677361  0.2671632  0.26672766 0.2663757  0.26608753
 0.26584843 0.26563287 0.26539844 0.2651351  0.26484606 0.2645813
 0.2643342  0.26410893 0.26388916 0.26360783 0.26320124 0.26259026
 0.26202282 0.26117343 0.2601451  0.2594341  0.25877917 0.25825098
 0.25784373 0.25754082 0.2572737  0.2570412  0.25674787 0.25642666
 0.25604948 0.25562617 0.2551533  0.25467905 0.25420147 0.253759
 0.2533061  0.25284025 0.25238234 0.25194296 0.25153708 0.2511769
 0.25088656 0.25063086 0.25040522 0.25016925 0.24990328 0.24960637
 0.24919955 0.2487319  0.2481499  0.24748784 0.24682972 0.24621089
 0.2456888  0.24532804 0.24507785 0.24490923 0.24478854 0.24457693
 0.24428396 0.24394138 0.24352151 0.2430578  0.24260756 0.24224047
 0.24190865 0.24164134 0.24141185 0.24118473 0.24098663 0.24073415
 0.24045381 0.24013032 0.23983024 0.23955216 0.23932154 0.23917404
 0.23910059 0.2390679  0.23902929 0.23894246 0.23882772 0.2386771
 0.23854274 0.2384308  0.238348   0.23824859 0.23817825 0.23811609
 0.23806876 0.23796079 0.2378554  0.23767035 0.237516   0.23734836
 0.23722543 0.23710261 0.23696402 0.23677835 0.23656248 0.23634045
 0.23615868 0.23602793 0.23597349 0.23599496 0.2361176  0.23630317
 0.23647746 0.23656097 0.23644526 0.23612568 0.2356291  0.23497434
 0.23431161 0.2337403  0.23328972 0.23300508 0.23286247 0.23283309
 0.23281392 0.23274738 0.23260315 0.23232985 0.23199001 0.23159549
 0.23119622 0.23085229 0.23063755 0.23052226 0.23045033 0.23035127
 0.23015127 0.22977856 0.2293245  0.22886966 0.22847408 0.22818497
 0.22806637 0.2280693  0.2281029  0.22805522 0.22785382 0.2274297
 0.22688894 0.22634768 0.22590597 0.22568603 0.22572887 0.22599255
 0.226283   0.22637609 0.22611053 0.22545552 0.2243613  0.2231446
 0.22209068 0.22171976 0.22199447 0.22266918 0.22297913 0.22163126]
