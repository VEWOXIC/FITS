Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=7, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:7
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  23532544.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 9.836608409881592
Epoch: 1, Steps: 59 | Train Loss: 0.7528287 Vali Loss: 1.4748544 Test Loss: 0.6430255
Validation loss decreased (inf --> 1.474854).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 9.904180526733398
Epoch: 2, Steps: 59 | Train Loss: 0.5791767 Vali Loss: 1.3180165 Test Loss: 0.5378333
Validation loss decreased (1.474854 --> 1.318017).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 10.053076982498169
Epoch: 3, Steps: 59 | Train Loss: 0.5231012 Vali Loss: 1.2584393 Test Loss: 0.4915550
Validation loss decreased (1.318017 --> 1.258439).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 8.773720741271973
Epoch: 4, Steps: 59 | Train Loss: 0.4943995 Vali Loss: 1.2218318 Test Loss: 0.4666623
Validation loss decreased (1.258439 --> 1.221832).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 10.205588340759277
Epoch: 5, Steps: 59 | Train Loss: 0.4775084 Vali Loss: 1.2022184 Test Loss: 0.4526246
Validation loss decreased (1.221832 --> 1.202218).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 12.625593900680542
Epoch: 6, Steps: 59 | Train Loss: 0.4671251 Vali Loss: 1.1952330 Test Loss: 0.4451498
Validation loss decreased (1.202218 --> 1.195233).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 13.589022397994995
Epoch: 7, Steps: 59 | Train Loss: 0.4606214 Vali Loss: 1.1906900 Test Loss: 0.4409897
Validation loss decreased (1.195233 --> 1.190690).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 14.171802282333374
Epoch: 8, Steps: 59 | Train Loss: 0.4561547 Vali Loss: 1.1834836 Test Loss: 0.4391707
Validation loss decreased (1.190690 --> 1.183484).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 14.392298936843872
Epoch: 9, Steps: 59 | Train Loss: 0.4531278 Vali Loss: 1.1870811 Test Loss: 0.4381651
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 12.470000505447388
Epoch: 10, Steps: 59 | Train Loss: 0.4509040 Vali Loss: 1.1885505 Test Loss: 0.4382020
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 12.387079954147339
Epoch: 11, Steps: 59 | Train Loss: 0.4491875 Vali Loss: 1.1879572 Test Loss: 0.4382926
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4379706084728241, mae:0.4425084590911865, rse:0.6300492882728577, corr:[0.24912058 0.26029992 0.25775713 0.2518602  0.25085917 0.25234938
 0.25269577 0.25189736 0.2508036  0.2506211  0.25093043 0.25089833
 0.25056684 0.25023618 0.25035658 0.25052845 0.24992426 0.24863367
 0.24750365 0.24722491 0.24776238 0.24794596 0.2473704  0.24617532
 0.24527177 0.24521406 0.24561681 0.24580415 0.2456053  0.24531509
 0.24543521 0.24600568 0.24667637 0.24699648 0.24679852 0.24641286
 0.24615493 0.24595828 0.2458142  0.2457786  0.24598595 0.24650869
 0.24710768 0.24767382 0.24811663 0.24851114 0.24885799 0.2489494
 0.24866864 0.2479794  0.24695744 0.2460503  0.24536434 0.2447523
 0.24423166 0.24379931 0.24341342 0.24309067 0.24293137 0.24290378
 0.2428719  0.24278234 0.24265008 0.24264891 0.24272455 0.24283288
 0.24285586 0.24271451 0.24272932 0.24275538 0.24266429 0.24237017
 0.2418897  0.24137494 0.24106054 0.24099272 0.24091463 0.24070795
 0.24050426 0.24043489 0.24041428 0.24031594 0.24005608 0.23973043
 0.23936827 0.23903993 0.23877072 0.23860376 0.23842242 0.23827815
 0.23813452 0.2380389  0.23782437 0.23757446 0.23744367 0.2377838
 0.2385745  0.23946746 0.24009961 0.24041472 0.24055277 0.2406039
 0.2407329  0.24090093 0.24093106 0.2407536  0.24053915 0.24048229
 0.24053037 0.24053507 0.2403363  0.24008243 0.23991275 0.23994176
 0.24002974 0.23991513 0.23966466 0.23941411 0.23928316 0.23925604
 0.23903653 0.23839065 0.2375953  0.23704313 0.23679918 0.23657517
 0.2362583  0.23599045 0.23579328 0.2357543  0.23577683 0.23573373
 0.23563191 0.23551223 0.23554353 0.2356672  0.23555718 0.23514454
 0.23455095 0.2341971  0.23426832 0.23438306 0.23415188 0.23371959
 0.23322202 0.23278497 0.23244242 0.23201156 0.2315328  0.23104465
 0.23085332 0.23111483 0.23153254 0.23167837 0.2315307  0.23136044
 0.23123717 0.23108704 0.23081511 0.23056595 0.230401   0.2305926
 0.23090632 0.23100923 0.23074792 0.23030008 0.22978896 0.22952898
 0.22952466 0.22962114 0.22962858 0.22961046 0.22956692 0.22947952
 0.22929321 0.22919321 0.22921106 0.22941501 0.22956797 0.22946194
 0.2290407  0.22867312 0.22880048 0.22944097 0.2300881  0.23030813
 0.23013261 0.23003922 0.23040295 0.2309149  0.23094249 0.23032014
 0.22939496 0.22871904 0.22844192 0.22818272 0.22761859 0.22679742
 0.22615212 0.22610164 0.22638266 0.22637613 0.22596812 0.22575028
 0.22614704 0.22708063 0.22774841 0.2275969  0.22690934 0.22632045
 0.22624606 0.22636597 0.2262051  0.22564432 0.22508915 0.22504623
 0.22539274 0.225564   0.22512527 0.22431326 0.22376886 0.2236119
 0.2236495  0.2236423  0.22348842 0.22332685 0.22326578 0.2233966
 0.22331892 0.22289541 0.22255063 0.22266775 0.22293912 0.2230357
 0.22271092 0.22212487 0.22175863 0.22185893 0.22227961 0.22247182
 0.22240914 0.2222566  0.2223447  0.22246647 0.22217526 0.2215556
 0.22110681 0.221255   0.22167498 0.22166166 0.22098926 0.22028133
 0.22016992 0.2205887  0.22104402 0.22089617 0.22034746 0.22008298
 0.220594   0.22129911 0.22149934 0.22081144 0.21974002 0.21902236
 0.21917242 0.21973054 0.21983409 0.21929593 0.21830232 0.21739043
 0.21689233 0.21659276 0.21639092 0.21664006 0.21721493 0.21760923
 0.21719521 0.21604715 0.21492273 0.21480772 0.21564773 0.21640699
 0.2159569  0.21441253 0.21318552 0.21320862 0.21418063 0.21489003
 0.21466617 0.21405038 0.21382442 0.21440092 0.2146886  0.21408981
 0.21297367 0.21235031 0.21269983 0.21342403 0.21336026 0.21261597
 0.21229152 0.21299718 0.21425746 0.21446992 0.21331662 0.21191618
 0.21200879 0.21320425 0.21415918 0.2132208  0.21079025 0.20874304
 0.20866185 0.20972487 0.20960623 0.20769623 0.2055752  0.20468903
 0.20511013 0.2051605  0.20412719 0.20290089 0.20263012 0.20327356
 0.20316696 0.20075546 0.19771251 0.19739439 0.19942996 0.20039546
 0.19625169 0.1880972  0.1832435  0.18675293 0.18993922 0.16553435]
