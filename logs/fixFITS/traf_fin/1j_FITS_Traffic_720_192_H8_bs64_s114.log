Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=6, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j192_H8', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:6
>>>>>>>start training : Traffic_720_j192_H8_FITS_custom_ftM_sl720_ll48_pl192_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=258, out_features=326, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9280140288.0
params:  84434.0
Trainable parameters:  84434
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 77.42628335952759
Epoch: 1, Steps: 88 | Train Loss: 0.7715139 Vali Loss: 0.6157503 Test Loss: 0.7064521
Validation loss decreased (inf --> 0.615750).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 81.85075449943542
Epoch: 2, Steps: 88 | Train Loss: 0.3790691 Vali Loss: 0.4114528 Test Loss: 0.4814983
Validation loss decreased (0.615750 --> 0.411453).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 68.13057065010071
Epoch: 3, Steps: 88 | Train Loss: 0.2768482 Vali Loss: 0.3531393 Test Loss: 0.4202903
Validation loss decreased (0.411453 --> 0.353139).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 61.11111855506897
Epoch: 4, Steps: 88 | Train Loss: 0.2491325 Vali Loss: 0.3380909 Test Loss: 0.4063152
Validation loss decreased (0.353139 --> 0.338091).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 59.52062249183655
Epoch: 5, Steps: 88 | Train Loss: 0.2422851 Vali Loss: 0.3335103 Test Loss: 0.4031390
Validation loss decreased (0.338091 --> 0.333510).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 63.73805522918701
Epoch: 6, Steps: 88 | Train Loss: 0.2403887 Vali Loss: 0.3316146 Test Loss: 0.4020912
Validation loss decreased (0.333510 --> 0.331615).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 63.639156103134155
Epoch: 7, Steps: 88 | Train Loss: 0.2396938 Vali Loss: 0.3309626 Test Loss: 0.4017384
Validation loss decreased (0.331615 --> 0.330963).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 59.51288866996765
Epoch: 8, Steps: 88 | Train Loss: 0.2392569 Vali Loss: 0.3308882 Test Loss: 0.4011214
Validation loss decreased (0.330963 --> 0.330888).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 51.85077714920044
Epoch: 9, Steps: 88 | Train Loss: 0.2390010 Vali Loss: 0.3298535 Test Loss: 0.4007616
Validation loss decreased (0.330888 --> 0.329854).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 57.285213470458984
Epoch: 10, Steps: 88 | Train Loss: 0.2388836 Vali Loss: 0.3296916 Test Loss: 0.4006208
Validation loss decreased (0.329854 --> 0.329692).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 64.51049280166626
Epoch: 11, Steps: 88 | Train Loss: 0.2386066 Vali Loss: 0.3294879 Test Loss: 0.4006028
Validation loss decreased (0.329692 --> 0.329488).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 67.53279829025269
Epoch: 12, Steps: 88 | Train Loss: 0.2386240 Vali Loss: 0.3291350 Test Loss: 0.4002744
Validation loss decreased (0.329488 --> 0.329135).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 69.28616523742676
Epoch: 13, Steps: 88 | Train Loss: 0.2384606 Vali Loss: 0.3290450 Test Loss: 0.4002324
Validation loss decreased (0.329135 --> 0.329045).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 72.58750319480896
Epoch: 14, Steps: 88 | Train Loss: 0.2384049 Vali Loss: 0.3293464 Test Loss: 0.4001557
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 69.71135091781616
Epoch: 15, Steps: 88 | Train Loss: 0.2384206 Vali Loss: 0.3288948 Test Loss: 0.4000382
Validation loss decreased (0.329045 --> 0.328895).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 72.50408363342285
Epoch: 16, Steps: 88 | Train Loss: 0.2382899 Vali Loss: 0.3280616 Test Loss: 0.3999893
Validation loss decreased (0.328895 --> 0.328062).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 69.44530749320984
Epoch: 17, Steps: 88 | Train Loss: 0.2382979 Vali Loss: 0.3286687 Test Loss: 0.3999055
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 67.99418377876282
Epoch: 18, Steps: 88 | Train Loss: 0.2381767 Vali Loss: 0.3288392 Test Loss: 0.3999031
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 68.5973744392395
Epoch: 19, Steps: 88 | Train Loss: 0.2381137 Vali Loss: 0.3286808 Test Loss: 0.3997967
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Traffic_720_j192_H8_FITS_custom_ftM_sl720_ll48_pl192_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.39903247356414795, mae:0.27559152245521545, rse:0.5213544964790344, corr:[0.2806867  0.28534243 0.2895785  0.28772712 0.289246   0.29151282
 0.2905554  0.2915509  0.2925178  0.29135007 0.29165867 0.29191336
 0.29032633 0.28983197 0.28998467 0.28912804 0.289184   0.29002857
 0.28991017 0.28996152 0.290393   0.28996518 0.28953123 0.28973126
 0.290451   0.2905763  0.2910693  0.29097176 0.29076603 0.2910325
 0.2907105  0.29039407 0.2907347  0.2904942  0.290052   0.2902823
 0.28988472 0.289056   0.28935412 0.28980395 0.2895715  0.28965905
 0.289796   0.2893171  0.2891353  0.2892729  0.28893647 0.28912282
 0.29009402 0.29019028 0.29023263 0.29041836 0.289874   0.28957167
 0.29012534 0.29003134 0.28950655 0.28971043 0.2896616  0.28914627
 0.28914124 0.28897947 0.2885095  0.28888577 0.2896605  0.28970715
 0.28945294 0.28931913 0.28896788 0.28865772 0.28867078 0.28900835
 0.28950927 0.28980243 0.28962314 0.28936186 0.28925046 0.28878483
 0.28832054 0.28843814 0.28843826 0.28808004 0.28800544 0.28787687
 0.2875462  0.28777048 0.28827557 0.28838852 0.28865308 0.289036
 0.28888583 0.2885743  0.2884227  0.2881118  0.2878608  0.28795615
 0.28790078 0.28819662 0.2886841  0.28849825 0.288278   0.2885867
 0.28857222 0.2881196  0.2879789  0.2877813  0.2872835  0.28718916
 0.28725427 0.28707504 0.28744736 0.28815177 0.28805974 0.28769118
 0.28770083 0.28752217 0.28716314 0.28700182 0.28691193 0.28691828
 0.28712767 0.28742787 0.28781077 0.2883632  0.2884467  0.2880695
 0.28812364 0.28824392 0.2878923  0.28780922 0.28799903 0.28765622
 0.28743368 0.2877441  0.28765848 0.28752366 0.28811297 0.28858644
 0.28872883 0.28911206 0.28911453 0.28857592 0.28846952 0.2886716
 0.28865018 0.28872794 0.28917292 0.2893764  0.28929564 0.28912878
 0.28873748 0.28823337 0.2878708  0.28780785 0.28827414 0.288818
 0.2887773  0.28877732 0.28912938 0.28901207 0.28881836 0.2892203
 0.28949583 0.28950536 0.2896758  0.28953105 0.2891013  0.28929287
 0.290024   0.2896589  0.2899554  0.29022902 0.28975683 0.28963083
 0.28945383 0.28868464 0.28857356 0.28870064 0.28825656 0.2891361
 0.2909153  0.29089123 0.29077518 0.2916444  0.29087758 0.29036522
 0.29142967 0.28959015 0.2876657  0.2888824  0.28592616 0.29259986]
