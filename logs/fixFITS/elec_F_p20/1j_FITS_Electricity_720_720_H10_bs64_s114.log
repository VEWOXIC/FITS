Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=4, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j720_H10', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=50, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:4
>>>>>>>start training : Electricity_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  8414822400.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6589374
	speed: 0.4116s/iter; left time: 2676.1075s
Epoch: 1 cost time: 54.07720923423767
Epoch: 1, Steps: 132 | Train Loss: 0.8137545 Vali Loss: 0.4816649 Test Loss: 0.5806739
Validation loss decreased (inf --> 0.481665).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4219459
	speed: 0.9249s/iter; left time: 5890.8550s
Epoch: 2 cost time: 55.91824984550476
Epoch: 2, Steps: 132 | Train Loss: 0.4926358 Vali Loss: 0.3275862 Test Loss: 0.4009639
Validation loss decreased (0.481665 --> 0.327586).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3240111
	speed: 0.9143s/iter; left time: 5702.5806s
Epoch: 3 cost time: 55.293922662734985
Epoch: 3, Steps: 132 | Train Loss: 0.3580753 Vali Loss: 0.2480149 Test Loss: 0.3054498
Validation loss decreased (0.327586 --> 0.248015).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2575290
	speed: 0.9008s/iter; left time: 5499.6613s
Epoch: 4 cost time: 54.578121185302734
Epoch: 4, Steps: 132 | Train Loss: 0.2875190 Vali Loss: 0.2081973 Test Loss: 0.2555170
Validation loss decreased (0.248015 --> 0.208197).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2538275
	speed: 0.9121s/iter; left time: 5448.1598s
Epoch: 5 cost time: 56.82422661781311
Epoch: 5, Steps: 132 | Train Loss: 0.2515650 Vali Loss: 0.1892601 Test Loss: 0.2301757
Validation loss decreased (0.208197 --> 0.189260).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2306989
	speed: 0.9434s/iter; left time: 5510.1919s
Epoch: 6 cost time: 55.81379175186157
Epoch: 6, Steps: 132 | Train Loss: 0.2340171 Vali Loss: 0.1810927 Test Loss: 0.2178036
Validation loss decreased (0.189260 --> 0.181093).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2292382
	speed: 0.9116s/iter; left time: 5204.4791s
Epoch: 7 cost time: 56.58451509475708
Epoch: 7, Steps: 132 | Train Loss: 0.2258182 Vali Loss: 0.1776637 Test Loss: 0.2118319
Validation loss decreased (0.181093 --> 0.177664).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2192130
	speed: 0.9151s/iter; left time: 5103.6638s
Epoch: 8 cost time: 54.6728880405426
Epoch: 8, Steps: 132 | Train Loss: 0.2219206 Vali Loss: 0.1761249 Test Loss: 0.2090770
Validation loss decreased (0.177664 --> 0.176125).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2162636
	speed: 0.8924s/iter; left time: 4858.8595s
Epoch: 9 cost time: 54.18537783622742
Epoch: 9, Steps: 132 | Train Loss: 0.2201855 Vali Loss: 0.1759907 Test Loss: 0.2077698
Validation loss decreased (0.176125 --> 0.175991).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2127883
	speed: 0.8826s/iter; left time: 4689.4651s
Epoch: 10 cost time: 54.13617396354675
Epoch: 10, Steps: 132 | Train Loss: 0.2193846 Vali Loss: 0.1755020 Test Loss: 0.2071067
Validation loss decreased (0.175991 --> 0.175502).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2181262
	speed: 0.9178s/iter; left time: 4755.0966s
Epoch: 11 cost time: 57.74667191505432
Epoch: 11, Steps: 132 | Train Loss: 0.2190297 Vali Loss: 0.1758899 Test Loss: 0.2066770
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2142608
	speed: 0.9495s/iter; left time: 4793.9165s
Epoch: 12 cost time: 55.61081504821777
Epoch: 12, Steps: 132 | Train Loss: 0.2186491 Vali Loss: 0.1754694 Test Loss: 0.2064670
Validation loss decreased (0.175502 --> 0.175469).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2150149
	speed: 0.9444s/iter; left time: 4643.8164s
Epoch: 13 cost time: 57.54802966117859
Epoch: 13, Steps: 132 | Train Loss: 0.2185347 Vali Loss: 0.1755703 Test Loss: 0.2063287
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2101608
	speed: 0.9287s/iter; left time: 4443.6990s
Epoch: 14 cost time: 54.502750873565674
Epoch: 14, Steps: 132 | Train Loss: 0.2184024 Vali Loss: 0.1755098 Test Loss: 0.2062106
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2238202
	speed: 0.8955s/iter; left time: 4166.7818s
Epoch: 15 cost time: 52.17475771903992
Epoch: 15, Steps: 132 | Train Loss: 0.2183422 Vali Loss: 0.1754784 Test Loss: 0.2061171
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2314316
	speed: 0.8058s/iter; left time: 3643.1900s
Epoch: 16 cost time: 47.8530330657959
Epoch: 16, Steps: 132 | Train Loss: 0.2182091 Vali Loss: 0.1752967 Test Loss: 0.2060418
Validation loss decreased (0.175469 --> 0.175297).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2222465
	speed: 0.8263s/iter; left time: 3626.6207s
Epoch: 17 cost time: 49.14798140525818
Epoch: 17, Steps: 132 | Train Loss: 0.2181435 Vali Loss: 0.1754942 Test Loss: 0.2060485
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2242414
	speed: 0.8150s/iter; left time: 3469.3556s
Epoch: 18 cost time: 47.82657837867737
Epoch: 18, Steps: 132 | Train Loss: 0.2180844 Vali Loss: 0.1755115 Test Loss: 0.2059958
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2131103
	speed: 0.8723s/iter; left time: 3598.2417s
Epoch: 19 cost time: 53.30843687057495
Epoch: 19, Steps: 132 | Train Loss: 0.2179313 Vali Loss: 0.1750812 Test Loss: 0.2059686
Validation loss decreased (0.175297 --> 0.175081).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2159420
	speed: 0.8824s/iter; left time: 3523.4051s
Epoch: 20 cost time: 51.567755699157715
Epoch: 20, Steps: 132 | Train Loss: 0.2179004 Vali Loss: 0.1753421 Test Loss: 0.2059273
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2198100
	speed: 0.8417s/iter; left time: 3249.8726s
Epoch: 21 cost time: 49.796393394470215
Epoch: 21, Steps: 132 | Train Loss: 0.2178287 Vali Loss: 0.1750812 Test Loss: 0.2059035
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2063030
	speed: 0.8431s/iter; left time: 3143.9551s
Epoch: 22 cost time: 49.28060269355774
Epoch: 22, Steps: 132 | Train Loss: 0.2178408 Vali Loss: 0.1752931 Test Loss: 0.2058972
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2231795
	speed: 0.8285s/iter; left time: 2980.0827s
Epoch: 23 cost time: 49.38035845756531
Epoch: 23, Steps: 132 | Train Loss: 0.2179112 Vali Loss: 0.1751250 Test Loss: 0.2059032
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2126078
	speed: 0.8421s/iter; left time: 2917.8020s
Epoch: 24 cost time: 49.787362575531006
Epoch: 24, Steps: 132 | Train Loss: 0.2178136 Vali Loss: 0.1751547 Test Loss: 0.2058488
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2278450
	speed: 0.8267s/iter; left time: 2755.4767s
Epoch: 25 cost time: 49.85379934310913
Epoch: 25, Steps: 132 | Train Loss: 0.2178493 Vali Loss: 0.1753040 Test Loss: 0.2058679
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2315290
	speed: 0.9013s/iter; left time: 2884.9341s
Epoch: 26 cost time: 52.64434814453125
Epoch: 26, Steps: 132 | Train Loss: 0.2177816 Vali Loss: 0.1749001 Test Loss: 0.2058717
Validation loss decreased (0.175081 --> 0.174900).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2219791
	speed: 0.8149s/iter; left time: 2500.9645s
Epoch: 27 cost time: 48.46700382232666
Epoch: 27, Steps: 132 | Train Loss: 0.2177472 Vali Loss: 0.1753002 Test Loss: 0.2058095
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2156884
	speed: 0.8433s/iter; left time: 2476.7908s
Epoch: 28 cost time: 49.96579313278198
Epoch: 28, Steps: 132 | Train Loss: 0.2176880 Vali Loss: 0.1751656 Test Loss: 0.2058141
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2213006
	speed: 0.8518s/iter; left time: 2389.3478s
Epoch: 29 cost time: 50.57314968109131
Epoch: 29, Steps: 132 | Train Loss: 0.2176942 Vali Loss: 0.1750145 Test Loss: 0.2057811
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2197079
	speed: 0.8544s/iter; left time: 2283.9340s
Epoch: 30 cost time: 51.09516787528992
Epoch: 30, Steps: 132 | Train Loss: 0.2176120 Vali Loss: 0.1749762 Test Loss: 0.2057977
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2190106
	speed: 0.8431s/iter; left time: 2142.2401s
Epoch: 31 cost time: 50.45667552947998
Epoch: 31, Steps: 132 | Train Loss: 0.2176558 Vali Loss: 0.1751281 Test Loss: 0.2057679
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2129652
	speed: 0.8943s/iter; left time: 2154.4290s
Epoch: 32 cost time: 53.73220777511597
Epoch: 32, Steps: 132 | Train Loss: 0.2176226 Vali Loss: 0.1747525 Test Loss: 0.2057771
Validation loss decreased (0.174900 --> 0.174752).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2170143
	speed: 0.8663s/iter; left time: 1972.4683s
Epoch: 33 cost time: 50.286176681518555
Epoch: 33, Steps: 132 | Train Loss: 0.2176769 Vali Loss: 0.1753490 Test Loss: 0.2057691
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2261249
	speed: 0.8457s/iter; left time: 1813.9568s
Epoch: 34 cost time: 50.13720703125
Epoch: 34, Steps: 132 | Train Loss: 0.2176577 Vali Loss: 0.1751642 Test Loss: 0.2057482
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2280063
	speed: 0.8303s/iter; left time: 1671.4428s
Epoch: 35 cost time: 49.953272104263306
Epoch: 35, Steps: 132 | Train Loss: 0.2175501 Vali Loss: 0.1748839 Test Loss: 0.2057479
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2076886
	speed: 0.8420s/iter; left time: 1583.8416s
Epoch: 36 cost time: 49.82854342460632
Epoch: 36, Steps: 132 | Train Loss: 0.2176402 Vali Loss: 0.1750676 Test Loss: 0.2057416
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2137058
	speed: 0.8331s/iter; left time: 1457.0970s
Epoch: 37 cost time: 49.30695867538452
Epoch: 37, Steps: 132 | Train Loss: 0.2175102 Vali Loss: 0.1749819 Test Loss: 0.2057454
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2246822
	speed: 0.8547s/iter; left time: 1382.0213s
Epoch: 38 cost time: 52.29119396209717
Epoch: 38, Steps: 132 | Train Loss: 0.2175536 Vali Loss: 0.1745901 Test Loss: 0.2057417
Validation loss decreased (0.174752 --> 0.174590).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2103685
	speed: 0.9132s/iter; left time: 1356.1645s
Epoch: 39 cost time: 55.26323628425598
Epoch: 39, Steps: 132 | Train Loss: 0.2174939 Vali Loss: 0.1751103 Test Loss: 0.2057198
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2109315
	speed: 0.8872s/iter; left time: 1200.4398s
Epoch: 40 cost time: 53.54484486579895
Epoch: 40, Steps: 132 | Train Loss: 0.2174454 Vali Loss: 0.1751084 Test Loss: 0.2057080
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2170068
	speed: 0.9000s/iter; left time: 1098.8411s
Epoch: 41 cost time: 53.95897030830383
Epoch: 41, Steps: 132 | Train Loss: 0.2175760 Vali Loss: 0.1751450 Test Loss: 0.2057061
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2192093
	speed: 0.8979s/iter; left time: 977.7839s
Epoch: 42 cost time: 54.75112724304199
Epoch: 42, Steps: 132 | Train Loss: 0.2176163 Vali Loss: 0.1750861 Test Loss: 0.2056933
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2154690
	speed: 0.8736s/iter; left time: 835.9919s
Epoch: 43 cost time: 51.82490587234497
Epoch: 43, Steps: 132 | Train Loss: 0.2175173 Vali Loss: 0.1749322 Test Loss: 0.2056989
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2109498
	speed: 0.8842s/iter; left time: 729.4628s
Epoch: 44 cost time: 54.54876470565796
Epoch: 44, Steps: 132 | Train Loss: 0.2174909 Vali Loss: 0.1750290 Test Loss: 0.2057043
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2097021
	speed: 0.9366s/iter; left time: 649.0846s
Epoch: 45 cost time: 56.04772973060608
Epoch: 45, Steps: 132 | Train Loss: 0.2174518 Vali Loss: 0.1747779 Test Loss: 0.2057046
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2184335
	speed: 0.8995s/iter; left time: 504.6149s
Epoch: 46 cost time: 54.061092138290405
Epoch: 46, Steps: 132 | Train Loss: 0.2174509 Vali Loss: 0.1748007 Test Loss: 0.2056882
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2115132
	speed: 0.8735s/iter; left time: 374.7382s
Epoch: 47 cost time: 51.906211376190186
Epoch: 47, Steps: 132 | Train Loss: 0.2174606 Vali Loss: 0.1750014 Test Loss: 0.2056997
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2094418
	speed: 0.8090s/iter; left time: 240.2751s
Epoch: 48 cost time: 49.21824765205383
Epoch: 48, Steps: 132 | Train Loss: 0.2175532 Vali Loss: 0.1750128 Test Loss: 0.2056911
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2312212
	speed: 0.8627s/iter; left time: 142.3422s
Epoch: 49 cost time: 52.59767460823059
Epoch: 49, Steps: 132 | Train Loss: 0.2174434 Vali Loss: 0.1748620 Test Loss: 0.2056837
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2228709
	speed: 0.8813s/iter; left time: 29.0826s
Epoch: 50 cost time: 54.180896282196045
Epoch: 50, Steps: 132 | Train Loss: 0.2174980 Vali Loss: 0.1750132 Test Loss: 0.2057011
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.0497355408796396e-05
>>>>>>>testing : Electricity_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.20405422151088715, mae:0.2933899760246277, rse:0.45060932636260986, corr:[0.44767734 0.44715294 0.44840252 0.45024395 0.45055568 0.45127177
 0.45161876 0.45130545 0.4513     0.45076373 0.45040697 0.4503037
 0.44995302 0.45006156 0.449917   0.4500912  0.45010063 0.45012385
 0.4503087  0.45013714 0.45042685 0.45043102 0.45069966 0.45083448
 0.450984   0.45143306 0.4514216  0.45148712 0.45122778 0.4510332
 0.45088774 0.45053735 0.4503819  0.45024315 0.45009673 0.45015478
 0.45005447 0.45004076 0.45014232 0.45021465 0.4502898  0.45022964
 0.4502846  0.45015547 0.45018727 0.45025083 0.4502594  0.4504015
 0.4503644  0.45067358 0.45066428 0.4504951  0.45047128 0.45019755
 0.4500965  0.44996324 0.4498051  0.44983333 0.44980717 0.44980517
 0.44994816 0.44995683 0.44996879 0.45002514 0.4500413  0.45010746
 0.4501636  0.45012692 0.45001984 0.44996744 0.44991854 0.44994906
 0.44991902 0.44998422 0.45003468 0.44983363 0.44974    0.4495323
 0.44941744 0.44936785 0.44921988 0.44923735 0.44932804 0.44927502
 0.44921392 0.4493573  0.44939625 0.44941714 0.44943878 0.44940364
 0.44944438 0.44941568 0.44931662 0.44932985 0.44933903 0.44928554
 0.44932163 0.44933203 0.44940326 0.44927132 0.449094   0.44910666
 0.44896117 0.4488827  0.4488366  0.4487813  0.44877687 0.44879824
 0.44879422 0.4487734  0.4487941  0.44882408 0.44881698 0.4488198
 0.44882503 0.44870064 0.448688   0.4486716  0.4486796  0.44877505
 0.4488474  0.44897366 0.44897926 0.4489377  0.44889784 0.44878024
 0.44875473 0.44865885 0.44864443 0.44865307 0.44859084 0.44861656
 0.44866237 0.44871074 0.44881147 0.44879863 0.44880804 0.4488403
 0.44882354 0.4488437  0.4488045  0.4488045  0.44878012 0.4487486
 0.44868714 0.44860432 0.44869632 0.44865015 0.4486325  0.44862637
 0.44854176 0.4485211  0.44851214 0.44850203 0.4484612  0.4484342
 0.44842175 0.44849935 0.44856685 0.44856712 0.4486008  0.44860104
 0.4486422  0.4486455  0.44855353 0.44845122 0.44838005 0.44817102
 0.44796187 0.44796446 0.44803783 0.44798145 0.44786155 0.44782242
 0.4477439  0.44765872 0.447593   0.44756106 0.44754887 0.4474319
 0.44744402 0.4475578  0.44752905 0.44752383 0.44747698 0.44741946
 0.44737026 0.44715694 0.44698933 0.4469328  0.44691062 0.44682536
 0.4467399  0.44688225 0.44694808 0.44689628 0.4468663  0.44675857
 0.44668922 0.44664717 0.4465331  0.44650677 0.4464406  0.446406
 0.44640002 0.4463617  0.44643795 0.44641986 0.4463928  0.4463877
 0.4463097  0.4461238  0.4459552  0.44595358 0.44593874 0.44583648
 0.4458249  0.44598278 0.44611007 0.44613293 0.44611263 0.44609836
 0.4460838  0.44604698 0.44603103 0.44594255 0.44590816 0.44588602
 0.4458668  0.44582927 0.4457879  0.44579569 0.44574744 0.44574454
 0.44568765 0.44550455 0.44538736 0.44529387 0.4452819  0.44520453
 0.44520244 0.44536978 0.44547796 0.4454606  0.44537795 0.44532192
 0.44530737 0.44527262 0.44528568 0.4452801  0.4451847  0.44517222
 0.4451555  0.44512886 0.44515887 0.44512844 0.445044   0.44504103
 0.44500777 0.4448967  0.4448725  0.4448099  0.44480938 0.44479975
 0.4448036  0.4449508  0.44497302 0.4449504  0.44494337 0.4449173
 0.4448903  0.44486034 0.44481036 0.44477537 0.44476795 0.44471964
 0.4446763  0.44469047 0.44471768 0.44468024 0.44469914 0.44467142
 0.4445866  0.44449523 0.4444159  0.44444105 0.4444586  0.44450477
 0.44470993 0.44481757 0.44483855 0.44485384 0.44487974 0.4448484
 0.44486427 0.44485888 0.44477874 0.4447431  0.4447027  0.44456068
 0.44457552 0.4446407  0.44463283 0.4446326  0.44450846 0.444498
 0.44453964 0.4445091  0.44452313 0.44451118 0.4444907  0.44445372
 0.4444316  0.44451323 0.44463888 0.44462276 0.4446241  0.4445914
 0.44452754 0.44459105 0.44456255 0.44443873 0.44438145 0.44435298
 0.4443022  0.44432792 0.4443563  0.44433758 0.44434458 0.44436446
 0.44433826 0.44419262 0.44410604 0.444024   0.4439565  0.4438065
 0.44351116 0.44353193 0.44362667 0.44356558 0.44344687 0.44337028
 0.44332656 0.44327366 0.44312674 0.4430121  0.44294703 0.4428843
 0.442851   0.44284546 0.4428288  0.44278193 0.44279873 0.44280502
 0.44274747 0.4425992  0.4425079  0.44250888 0.44250247 0.44247934
 0.44233638 0.4424173  0.44252577 0.44248712 0.44248843 0.4424378
 0.44237503 0.44229597 0.44219851 0.44215685 0.44206375 0.44194752
 0.44193512 0.44188118 0.44188362 0.44189405 0.4418041  0.44180462
 0.44174388 0.44155768 0.44142702 0.44142464 0.44144398 0.44136322
 0.44142517 0.44152695 0.4416232  0.4416801  0.44169974 0.44170347
 0.44162712 0.44163424 0.44159847 0.44149378 0.44140613 0.44130483
 0.44124496 0.44127145 0.4411961  0.4411826  0.44118094 0.44110644
 0.44112945 0.44098765 0.44087672 0.44077533 0.4407484  0.4408754
 0.44082084 0.44091547 0.44109216 0.44112808 0.4411102  0.44115442
 0.44117522 0.4411149  0.44101432 0.44096777 0.4409251  0.44084883
 0.4408132  0.4407695  0.44084606 0.4408209  0.44073105 0.44073352
 0.44069618 0.44054857 0.44046733 0.44054282 0.44056076 0.44059595
 0.44062674 0.44073707 0.44092155 0.44090196 0.44094583 0.4409491
 0.44087642 0.44087547 0.4407966  0.4406808  0.44062212 0.44052786
 0.4404906  0.4404891  0.44048724 0.44052473 0.44043925 0.44045684
 0.44039595 0.44021076 0.44015253 0.44012928 0.4402233  0.44029728
 0.44055402 0.4406876  0.4407562  0.44086486 0.44083065 0.440863
 0.4408713  0.44080612 0.44077718 0.4406354  0.44051614 0.44044787
 0.4403484  0.44038856 0.44039685 0.4403292  0.4403387  0.4403249
 0.4403419  0.44032398 0.44038397 0.4404469  0.44045985 0.44050774
 0.44043723 0.44056857 0.4407117  0.44070876 0.44079277 0.4408115
 0.44078517 0.44079033 0.44071954 0.4406207  0.44055125 0.44050327
 0.44041142 0.44038153 0.44046435 0.44044796 0.44040164 0.44036606
 0.440379   0.44031858 0.44018555 0.44016027 0.44014084 0.44003323
 0.43978095 0.4397681  0.4398842  0.4398237  0.4397804  0.4397566
 0.4396833  0.43963858 0.43944675 0.4392732  0.43914786 0.4390297
 0.43904564 0.43896624 0.43894947 0.43899763 0.43881798 0.43880153
 0.43876556 0.43856257 0.43851885 0.43845198 0.43848166 0.43852496
 0.43840116 0.4384409  0.43858358 0.43857235 0.43853807 0.43858343
 0.43849322 0.43838942 0.43827266 0.4380763  0.43795374 0.43778354
 0.4377517  0.43779445 0.4376333  0.43761542 0.43753463 0.43746212
 0.43750668 0.4372779  0.4372507  0.43733555 0.4373405  0.43738803
 0.43736827 0.43748954 0.4376034  0.43758202 0.4375745  0.43747526
 0.4374195  0.43740848 0.43723843 0.43705457 0.43687725 0.43671623
 0.4365528  0.43649685 0.43649885 0.4363348  0.43622836 0.43608874
 0.4360281  0.43595225 0.43587193 0.43591034 0.43592554 0.4360473
 0.43603978 0.4361314  0.43635476 0.43635076 0.4363577  0.43631393
 0.43624282 0.4361559  0.43594322 0.43585047 0.43568432 0.4354582
 0.43549824 0.43536612 0.43529308 0.43531302 0.43521813 0.43531024
 0.43519706 0.43506125 0.43510482 0.43515533 0.43525994 0.43526787
 0.4354114  0.43556747 0.43567494 0.43567058 0.4356236  0.43563092
 0.43551895 0.43541613 0.4352675  0.43506193 0.43502527 0.43483087
 0.4346728  0.4347774  0.4346754  0.43473902 0.4347197  0.43463993
 0.4347391  0.43460953 0.43465728 0.43466234 0.43475142 0.43492842
 0.43515995 0.43539852 0.4354335  0.43549204 0.4354511  0.43532386
 0.4352947  0.43512717 0.43496218 0.43479457 0.4345438  0.43445498
 0.43435463 0.4343263  0.43436468 0.4342994  0.43442962 0.4344114
 0.43450063 0.43453902 0.4345257  0.4347329  0.43466923 0.43482113
 0.43484473 0.43488407 0.43510768 0.43504253 0.43498358 0.43485427
 0.4347406  0.43461558 0.43439114 0.43429655 0.43408996 0.43394843
 0.43399432 0.4339064  0.4340059  0.43403655 0.43408749 0.43427488
 0.4342789  0.43434134 0.4342624  0.4342874  0.43441296 0.43429756
 0.43409178 0.4339644  0.4340231  0.43384582 0.4336065  0.43348083
 0.43308467 0.43294582 0.4325742  0.43227816 0.43226343 0.4319588
 0.43207008 0.43213454 0.4322027  0.43239284 0.43233386 0.43261066
 0.43264252 0.43250704 0.43252733 0.43240887 0.43270335 0.43251812
 0.43225333 0.4322465  0.43199036 0.43198806 0.43147856 0.43143386
 0.4311829  0.43088016 0.43078184 0.430457   0.43092477 0.43077427
 0.4310784  0.43146995 0.43148226 0.4319565  0.43163824 0.43153918
 0.43133652 0.43018532 0.42838603 0.42928877 0.4246858  0.4277322 ]
